Week 5:

There are more optimized functions than Gradient Descent (such as fminunc in Matlab) that takes a Cost Function and initial thetas, and calculates the result thetas that minimize the cost - get the minimized value of cost function.


Such a Cost Function for a 3-layer Neural Network could be as follows:
function [J grad] = nnCostFunction(unrolled_thetas, ...
                                   input_layer_size, ...
                                   hidden_layer_size, ...
                                   num_labels, ...
                                   X, y, lambda)
Given a concrete initial theta (unrolled) vector, concrete X and y training data, calculate the concrete value of J (a.k.a. cost), and concrete value of gradients (a.k.a. partial derivative) for every theta. We may also calculate Regularized cost to avoid overfitting of training data, by applying lambda.